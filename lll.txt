52
0.627341383832673 0.4700101097054401
1.155370199009371 0.8986689111799339
Inverse Propensity Score Weight STD and Mean 17.586344526827567 0.7985576827703699
0.23356607578937746 56.50051302599464
1.281264686475644 2.0181202111399514
Optimal Policy Value: 1.3919306300848497
Epoch 99
Loss 0.480529
Epoch 199
Loss 0.363121
Epoch 299
Loss 0.342211
Epoch 399
Loss 0.331723
Epoch 499
Loss 0.321921
290.24207
53.070576
Epoch 2000, Loss 0.001832
Epoch 4000, Loss 0.001683
Epoch 6000, Loss 0.001192
Epoch 8000, Loss 0.000812
Epoch 10000, Loss 0.000706
Epoch 12000, Loss 0.000563
Epoch 14000, Loss 0.000513
Epoch 16000, Loss 0.000476
Epoch 18000, Loss 0.000457
Epoch 20000, Loss 0.000452
Iter 0 DM Search Optimized Policy Value: 1.3185619158278463
Train 1.0209035650709664
Test 27.966702093690373
290.24207
53.070576
Epoch 2000, Loss 0.000396
Epoch 4000, Loss 0.000258
Epoch 6000, Loss 0.000176
Epoch 8000, Loss 0.000136
Epoch 10000, Loss 0.000102
Epoch 12000, Loss 0.000096
Epoch 14000, Loss 0.000087
Epoch 16000, Loss 0.000082
Epoch 18000, Loss 0.000080
Epoch 20000, Loss 0.000080
Iter 1 DM Search Optimized Policy Value: 1.361766962073602
Train 0.4567924337603109
Test 34.26842985546751
290.24207
53.070576
Epoch 2000, Loss 0.000081
Epoch 4000, Loss 0.000087
Epoch 6000, Loss 0.000066
Epoch 8000, Loss 0.000059
Epoch 10000, Loss 0.000049
Epoch 12000, Loss 0.000047
Epoch 14000, Loss 0.000045
Epoch 16000, Loss 0.000043
Epoch 18000, Loss 0.000045
Epoch 20000, Loss 0.000043
Iter 2 DM Search Optimized Policy Value: 1.3618963144706377
Train 0.34332390971049764
Test 35.489358657264596
290.24207
53.070576
Epoch 2000, Loss 0.000076
Epoch 4000, Loss 0.000070
Epoch 6000, Loss 0.000045
Epoch 8000, Loss 0.000040
Epoch 10000, Loss 0.000036
Epoch 12000, Loss 0.000035
Epoch 14000, Loss 0.000034
Epoch 16000, Loss 0.000033
Epoch 18000, Loss 0.000034
Epoch 20000, Loss 0.000034
Iter 3 DM Search Optimized Policy Value: 1.3622971690861316
Train 0.303651936258313
Test 36.35986320671202
290.24207
53.070576
Epoch 2000, Loss 0.000099
Epoch 4000, Loss 0.000042
Epoch 6000, Loss 0.000033
Epoch 8000, Loss 0.000036
Epoch 10000, Loss 0.000033
Epoch 12000, Loss 0.000031
Epoch 14000, Loss 0.000030
Epoch 16000, Loss 0.000030
Epoch 18000, Loss 0.000030
Epoch 20000, Loss 0.000029
Iter 4 DM Search Optimized Policy Value: 1.3623395086512424
Train 0.2834129347070641
Test 36.84211852457109
290.24207
53.070576
Epoch 2000, Loss 0.000036
Epoch 4000, Loss 0.000055
Epoch 6000, Loss 0.000042
Epoch 8000, Loss 0.000032
Epoch 10000, Loss 0.000033
Epoch 12000, Loss 0.000030
Epoch 14000, Loss 0.000029
Epoch 16000, Loss 0.000028
Epoch 18000, Loss 0.000027
Epoch 20000, Loss 0.000027
Iter 5 DM Search Optimized Policy Value: 1.3622670028515504
Train 0.2729113412086936
Test 37.110436095905314
0.006102483107732785
0.006102483107732785
0.511343070548767
DM Search Optimized Policy Value: 1.3622670028515504
[1433 1678 1153  564  482  256  281   88 1221  937 1895  639  171 1831
  217 1238 1149 1403 1481 1154]
[[ 5.29535017  4.52389937]
 [ 4.65760785  3.81672079]
 [ 4.15920539  3.30411751]
 [ 5.67492011  4.81294338]
 [ 4.97517013  4.08973199]
 [ 3.62215357  2.73348425]
 [ 5.37936154  4.47620745]
 [ 3.85704686  2.87895021]
 [ 4.77027941  3.65544393]
 [ 3.79931429  2.65410276]
 [ 5.0442856   3.87516686]
 [ 4.71008204  3.41192824]
 [ 4.03257697  2.71079725]
 [ 7.16586962  5.840935  ]
 [ 4.89804461  3.05084919]
 [ 5.76314261  3.8746278 ]
 [ 7.41912594  5.39913768]
 [ 5.19166553  3.08312367]
 [10.35137646  7.89049059]
 [ 7.79058497  4.08986887]]
Train: 0.006102483107732785
Test: 0.511343070548767
[1.3622670028515504]
Value: 1.3622670028515504
